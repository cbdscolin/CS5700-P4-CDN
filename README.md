Our high-level approach was to implement the DNS server and the HTTP server separately initially. We began by first implementing a simple DNS server for the milestone. For the HTTP server, we implemented a basic HTTP server, that takes a GET request from the client for a specific page, and then queries the origin server for that specific page. Once it gets that page, it will then return it in HTML format to the user. The HTTP server also includes a basic LRU (least recently used) caching algorithm that caches each webpage it sees, removing the least recently used page when it runs out of space (the space is determined by a predetermined number). If the page the client asks for is in the cache, it will not query the origin server, and will rather return the cached copy to the client. The HTTP server is also designed to respond to requests for /grading/beacon with a 204 response code.

We then designed the three scripts for starting, running, and stopping these servers (deployCDN, runCDN, and stopCDN, respectively). These scripts are simple bash scripts that copy files to the remote servers, run the dns and HTTP server code, and then kill the processes when they are no longer needed.

For mapping client IPs to replica IPs, we implemented a system using geolocation of all the IP addresses involved. This system uses an online API (freegeoip.app, an IP geolocation service with high rate limits which can be extended by using multiple accounts and at no cost) to determine where IPs are geographically. Upon construction, the DNS Server uses this API to find the geographic locations of every replica server, and then stores these locations. To map a client to the nearest replica server, it determines where the client is geographically, and determines the distances between the replica servers and the client (in kilometers). Then, it caches this list of distances from the client, so that if the client queries again, the DNS server will not have to use the geolocation API again. Thus, for each client request, the DNS server will be able to produce a list of replica servers in order of distance to the client.

We also implemented active measurement to ensure that the replica server IPs we gave to the clients were not overloaded. We implemented a system that uses the scamper tool to determine if any of the replica servers are overloaded, by sending a trace command to each of the replicas. We set a time limit on this trace command of 5 seconds, such that if any replicas have too high load to provide a fast response, they will not appear in this output. For servers that may naturally have a long response time due to geographic distance, we assume that they are always functioning. This design decision was made because some replica servers, generally those on the other side of the planet, would take multiple seconds to respond to trace commands. Since we could not use subprocess, this means that some clients would have to wait for the DNS server to finish using scamper on these far away replicas, which would cause time outs for the client’s dig requests. We found that a good estimate for servers that could easily be reached within the time of a client’s dig request was servers within 12,000 kilometers of the DNS server. Thus, this step marks any servers within 12,000 km of the DNS server that do not respond within the 5 second time limit as overloaded. To choose a replica IP, we then iterate through the list of replica IPs, ordered by distance to the client. We select the first non-overloaded IP in the list, and return this to the client. 

We also further optimized the HTTP servers running on the replicas by implementing a compression algorithm for storing pages, and by measuring the maximum number of pages that could be stored given the memory requirements of the replica servers. 

To evaluate the effectiveness of our system, we tested the accuracy of the DNS server using our local machines and the build server (cs5700cdnproject.ccs.neu.edu). We performed dig commands, and saw that the returned replica IP was the geographically closest replica on the given list of replica IPs. We also checked the calculated distances for each replica and our machines (the client), to evaluate the accuracy of our geolocation and distance functionality. 

To evaluate the accuracy of our active measurement system, we sent multiple requests in a script to specific replica IPs (using our specific group’s port), to simulate a node having high network load (often ~1000 requests per second). We then tested our scamper and active measurement functionality by seeing if the returned replica IP was not the overloaded replica. For example, we overloaded the 45.33.99.146 replica, which is the closest replica to our machines, and checked that the DNS server did not return this IP to us, and rather returned the next closest replica (50.116.6.217). 

To evaluate the accuracy of our HTTP servers, we used wget commands to get specific web pages from the replica servers, and used browsers to perform HTTP get requests. 

With more time, we would further optimize our use of scamper for active measurement. We would focus on using scamper in a faster way, such that we could test all of the replica IPs, regardless of geographical location or network distance, within a reasonable timeframe for the client to wait during a dig request. We would also work on expanding our use of active measurement in the replica IP decision process. At the moment, we use active measurement to identify overloaded servers; however, we could expand this to actually determine the fastest server for the client. We would also further optimize the HTTP servers, using additional compression algorithms in caching, or implementing a system to check which replica servers have which pages cached, to further decrease latencies for clients. 

For the milestone, John worked primarily on the DNS server implementation, and Colin worked primarily on the HTTP server implementation. Colin designed the deployCDN, runCDN, and stopCDN scripts, and John made bug fixes for full functionality with the dig and wget commands. After the milestone, Colin and John both designed the DNS server strategy, ultimately deciding to use geolocation and active measurement. Colin worked on implementing IP geolocation via the API, and John implemented DNS caching, and together we pair programmed the active measurement portion. We both created documentation for the entire project.
